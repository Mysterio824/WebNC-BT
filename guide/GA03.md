# Centralized API Logging with the ELK Stack

This guide details a centralized logging system for a Spring Boot API. It is designed to meet the seminar requirements by capturing, storing, rotating, and searching API request logs.

We use the **ELK Stack** (Elasticsearch, Logstash, Kibana) and **Filebeat** to create a robust, searchable, and scalable logging pipeline.

## What This Guide Covers

This implementation directly addresses the four seminar requirements:

* **ðŸ‘‰ 1. Log request data:** We use a Spring Boot `Interceptor` to log request/response data for all API endpoints.
* **ðŸ‘‰ 2. Save logs to a database:** Logs are processed and stored in **Elasticsearch**, a powerful search database.
* **ðŸ‘‰ 3. Implement log rotation:** We implement **daily rotation** at the Logstash level and propose **size-based rotation** using Elasticsearch's ILM.
* **ðŸ‘‰ 4. Search through logs:** We use **Kibana** to search, filter, and visualize all log data for debugging.

## Tech Stack

* **Spring Boot 3**: The application generating the logs.
* **Logstash Logback Encoder**: A Java library to write logs as structured JSON.
* **ELK Stack**:
    * **Elasticsearch 8.10.4**: The database for storing and indexing logs.
    * **Logstash 8.10.4**: The server for parsing and processing logs.
    * **Kibana 8.10.4**: The web interface for searching and visualizing logs.
* **Filebeat 8.10.4**: A lightweight "shipper" that sends logs from our app to Logstash.
* **Docker & Docker Compose**: To run the entire ELK stack.

-----

## Project File Structure

Before you begin, your project must have the following files and directories. This is crucial for Docker Compose to link everything together.

```
.
â”œâ”€â”€ docker-compose.yml        <-- Runs the entire ELK stack
â”œâ”€â”€ filebeat/
â”‚   â””â”€â”€ filebeat.yml          <-- Config for Filebeat
â”œâ”€â”€ logstash/
â”‚   â””â”€â”€ pipeline/
â”‚       â””â”€â”€ logstash.conf     <-- Config for the Logstash pipeline
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ my-app.log            <-- Your Spring Boot app writes to this file
â””â”€â”€ src/                      <-- Your Spring Boot application source code
    â””â”€â”€ main/
        â”œâ”€â”€ java/
        â”‚   â””â”€â”€ com/webnc/bt/
        â”‚       â”œâ”€â”€ interceptor/
        â”‚       â”‚   â””â”€â”€ ApiLoggingInterceptor.java
        â”‚       â””â”€â”€ WebConfig.java
        â””â”€â”€ resources/
            â””â”€â”€ logback-spring.xml  <-- Config for Spring Boot logging
```

-----

## How to Set Up and Run

This is a multi-part setup:

1.  Configure the Spring Boot app to *create* JSON logs.
2.  Configure the ELK stack to *receive* those logs.
3.  Run everything.

### Step 1: Configure the Spring Boot Application

Your app must be configured to write logs in JSON format to the `logs/my-app.log` file.

**A. Add Maven Dependency (pom.xml)**

Add the Logstash encoder to your `pom.xml`:

```xml
<dependency>
    <groupId>net.logstash.logback</groupId>
    <artifactId>logstash-logback-encoder</artifactId>
    <version>7.4</version>
</dependency>
```

**B. Configure Logback (`src/main/resources/logback-spring.xml`)**

This file tells Spring Boot to write to `logs/my-app.log` using the JSON format.

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <include resource="org/springframework/boot/logging/logback/base.xml"/>

    <appender name="ELK_JSON_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>logs/my-app.log</file>
        
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <appName>my-spring-boot-app</appName>
        </encoder>

        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>logs/my-app.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
    </appender>

    <logger name="api.logger" level="INFO" additivity="false">
        <appender-ref ref="ELK_JSON_FILE" />
    </logger>
</configuration>
```

**C. Create the Interceptor (`ApiLoggingInterceptor.java`)**

This class intercepts all API requests, times them, and writes the log.

```java
package com.webnc.bt.interceptor;

import jakarta.servlet.http.HttpServletRequest;
import jakarta.servlet.http.HttpServletResponse;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.stereotype.Component;
import org.springframework.web.servlet.HandlerInterceptor;
import java.util.UUID;
import static net.logstash.logback.argument.StructuredArguments.kv;

@Component
public class ApiLoggingInterceptor implements HandlerInterceptor {

    private static final Logger log = LoggerFactory.getLogger("api.logger");
    private static final String REQUEST_ID_ATTR = "request_id";
    private static final String START_TIME_ATTR = "startTime";

    @Override
    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) {
        long startTime = System.currentTimeMillis();
        String requestId = UUID.randomUUID().toString();
        request.setAttribute(START_TIME_ATTR, startTime);
        request.setAttribute(REQUEST_ID_ATTR, requestId);

        // Log when the request arrives
        log.info("API Request Started",
                kv("trace_id", requestId),
                kv("http_method", request.getMethod()),
                kv("http_path", request.getRequestURI()),
                kv("client_ip", request.getRemoteAddr())
        );
        return true;
    }

    @Override
    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) {
        long startTime = (Long) request.getAttribute(START_TIME_ATTR);
        String requestId = (String) request.getAttribute(REQUEST_ID_ATTR);
        long duration = System.currentTimeMillis() - startTime;

        // Log when the request is finished
        log.info("API Request Finished",
                kv("trace_id", requestId),
                kv("http_method", request.getMethod()),
                kv("http_path", request.getRequestURI()),
                kv("http_status", response.getStatus()),
                kv("duration_ms", duration),
                kv("exception", ex != null ? ex.getMessage() : null)
        );
    }
}
```

**D. Register the Interceptor (`WebConfig.java`)**

```java
package com.webnc.bt;

import com.webnc.bt.interceptor.ApiLoggingInterceptor;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.annotation.Configuration;
import org.springframework.web.servlet.config.annotation.InterceptorRegistry;
import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;

@Configuration
public class WebConfig implements WebMvcConfigurer {

    @Autowired
    private ApiLoggingInterceptor apiLoggingInterceptor;

    @Override
    public void addInterceptors(InterceptorRegistry registry) {
        // Apply our interceptor to all API paths
        registry.addInterceptor(apiLoggingInterceptor);
    }
}
```

### Step 2: Configure the ELK Stack

These files define the logging pipeline.

**A. `filebeat/filebeat.yml`**

This tells Filebeat to watch `my-app.log` and send any new lines to Logstash.

```yaml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    # This path is *inside* the Filebeat container
    - /var/log/my-app/*.log 

  # Tell Filebeat your logs are JSON
  json.keys_under_root: true
  json.add_error_key: true
  json.message_key: message

# ----------------- Output -----------------
# Send logs to Logstash
output.logstash:
  hosts: ["logstash:5044"]
```

**B. `logstash/pipeline/logstash.conf`**

This tells Logstash to listen for Filebeat, parse the JSON, and send it to Elasticsearch.

```ruby
input {
  beats {
    port => 5044
  }
}

filter {
  # The log message is already JSON, so just parse it
  json {
    source => "message"
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    # === REQUIREMENT 3: Daily Log Rotation ===
    # This creates a new index every day
    index => "api-logs-%{+YYYY.MM.dd}" 
  }
}
```

**C. `docker-compose.yml`**

This file starts and connects all the services.

```yaml
version: '3.8'

services:
  elasticsearch:
    image: elasticsearch:8.10.4
    container_name: es01
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false # For demo only
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - esdata:/usr/share/elasticsearch/data

  logstash:
    image: logstash:8.10.4
    container_name: logstash01
    ports:
      - "5044:5044"
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline:ro
    depends_on:
      - elasticsearch

  kibana:
    image: kibana:8.10.4
    container_name: kibana01
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

  filebeat:
    image: elastic/filebeat:8.10.4
    container_name: filebeat01
    user: root
    volumes:
      - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      # Mount the local logs directory into the container
      - ./logs:/var/log/my-app:ro 
    depends_on:
      - logstash

volumes:
  esdata:
```

### Step 3: Run the System

Now you can start everything.

1.  **Change File Permissions:**
    Filebeat and your app need to access the log files and configs. Run these commands from your project root.

    ```bash
    # Give your user ownership of the logs directory
    sudo chown -R $USER:$USER ./logs

    # Give root ownership of the Filebeat config (required by Filebeat)
    sudo chown root:root ./filebeat/filebeat.yml
    sudo chmod go-w ./filebeat/filebeat.yml
    ```

2.  **Start the ELK Stack:**
    In your terminal, run:

    ```bash
    docker-compose up -d
    ```

3.  **Verify the Stack:**
    Wait one minute, then run `docker-compose ps`. All 4 containers (`es01`, `logstash01`, `kibana01`, `filebeat01`) should show `State` as `Up`.

4.  **Run the Spring Boot Application:**
    In a **new terminal**, start your API server:

    ```bash
    # On macOS/Linux
    ./mvnw spring-boot:run

    # On Windows
    ./mvnw.cmd spring-boot:run
    ```

Your full logging pipeline is now running.

-----

## How to Use the Logging System

### ðŸ‘‰ 1. Generate & Store Logs (Reqs 1 & 2)

This step is automatic. Simply use your API. The `ApiLoggingInterceptor` will catch the request and log it.

**Example:** Hit an endpoint from your Sakila API (GA1).

```bash
curl -X GET http://localhost:8888/api/actors/5
```

This single command triggers the following flow:

1.  **Spring Boot** writes two JSON logs (e.g., `API Request Started...` and `API Request Finished...`) to `logs/my-app.log`.
2.  **Filebeat** sees the new lines and sends them to Logstash.
3.  **Logstash** parses the JSON and sends the structured data to Elasticsearch.
4.  **Elasticsearch** stores and indexes the log in `api-logs-2025.10.25`.

### ðŸ‘‰ 2. Search Logs (Req 4)

This is the "debugging" part, done in **Kibana**.

1.  Open Kibana in your browser: **`http://localhost:5601`**

2.  **Step A: Create the Index Pattern**

    * Click the main menu (â˜°) \> **Stack Management** \> **Kibana** \> **Index Patterns**.
    * Click **"Create index pattern"**.
    * In the name field, type **`api-logs-*`**. It should find your daily index.
    * Click "Next step", select `@timestamp` as the time field, and create it.

3.  **Step B: Discover Your Logs**

    * Click the main menu (â˜°) \> **Discover**.
    * You will see all your logs in a list, parsed into fields.
    * Use the search bar at the top (it uses **KQL**) to filter for what you need.

    **Example Searches:**

    * **Find all server errors:**
      `http_status >= 500`
    * **Find all slow requests:**
      `duration_ms > 500`
    * **Find all requests to a specific endpoint:**
      `http_path: "/api/actors/5"`
    * **See the full log for a single request:**
      `trace_id: "your-trace-id-from-the-log"`

### ðŸ‘‰ 3. Log Rotation (Req 3)

Your system already has two types of rotation implemented and one more to propose.

* **Method 1: File Rotation (Implemented in Spring Boot)**

    * **How:** The `logback-spring.xml` configuration automatically renames `my-app.log` to `my-app.2025-10-25.log` at midnight and deletes files older than 30 days.
    * **Pro:** Keeps the local disk clean.

* **Method 2: Daily Index Rotation (Implemented in ELK)**

    * **How:** The `logstash.conf` setting `index => "api-logs-%{+YYYY.MM.dd}"` creates a new database index every day.
    * **Pro:** This is the *key* for managing data in Elasticsearch. It allows you to delete old *indices* (e.g., all of September) instead of one log at a time.

* **Method 3: Propose Size & Lifecycle Rotation (For the Seminar)**

    * **How:** Use **Index Lifecycle Management (ILM)** in Kibana.
    * **Steps to Propose:**
        1.  In Kibana, go to **Stack Management** \> **Elasticsearch** \> **Index Lifecycle Policies**.
        2.  Create a policy (e.g., `api_logs_policy`).
        3.  Define phases:
            * **Hot:** Rollover the index when it hits **50GB** or is **1 day** old (whichever comes first).
            * **Warm (Optional):** After 7 days, move data to slower storage.
            * **Delete:** After 90 days, **delete the index automatically**.
    * **Pro:** This is the modern, professional solution that handles both time *and* size, and automates data cleanup.